{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_SentimentalAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2xLM7punx4b19NeZYb99/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinChiew/MachineLearning/blob/main/NLP_SentimentalAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ldrqoSvCEc"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5uEN0rFu7OI"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9v5RNFGXcQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e526d4de-0c7b-4edf-fcb8-568b23110c14"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE9lMCiVyB05"
      },
      "source": [
        "# Load Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq497Y03yA91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01791e21-e841-478d-ad81-33e6b0a71e23"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/stopwords.txt\n",
        "!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/positive.review\n",
        "!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/unlabeled.review\n",
        "!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/negative.review"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-14 11:29:58--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2488 (2.4K) [text/plain]\n",
            "Saving to: ‘stopwords.txt.1’\n",
            "\n",
            "\rstopwords.txt.1       0%[                    ]       0  --.-KB/s               \rstopwords.txt.1     100%[===================>]   2.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-14 11:29:58 (44.3 MB/s) - ‘stopwords.txt.1’ saved [2488/2488]\n",
            "\n",
            "--2021-05-14 11:29:58--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/positive.review\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1105010 (1.1M) [text/plain]\n",
            "Saving to: ‘positive.review.1’\n",
            "\n",
            "positive.review.1   100%[===================>]   1.05M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-05-14 11:29:59 (16.3 MB/s) - ‘positive.review.1’ saved [1105010/1105010]\n",
            "\n",
            "--2021-05-14 11:29:59--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/unlabeled.review\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14584772 (14M) [text/plain]\n",
            "Saving to: ‘unlabeled.review.1’\n",
            "\n",
            "unlabeled.review.1  100%[===================>]  13.91M  57.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-14 11:29:59 (57.6 MB/s) - ‘unlabeled.review.1’ saved [14584772/14584772]\n",
            "\n",
            "--2021-05-14 11:29:59--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/nlp_class/electronics/negative.review\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1113512 (1.1M) [text/plain]\n",
            "Saving to: ‘negative.review.1’\n",
            "\n",
            "negative.review.1   100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-14 11:29:59 (19.9 MB/s) - ‘negative.review.1’ saved [1113512/1113512]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cPsSEpIvBb0"
      },
      "source": [
        "stop_words = None\n",
        "\n",
        "with open('stopwords.txt') as f:\n",
        "    stop_words = set(w.rstrip() for w in f)     # use set to get unique list of vocab\n",
        "\n",
        "with open('positive.review') as f:\n",
        "    positive_reviews = BeautifulSoup(f.read()).findAll('review_text')\n",
        "\n",
        "with open('negative.review') as f:\n",
        "    negative_reviews = BeautifulSoup(f.read()).findAll('review_text')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm4BetrwomJN"
      },
      "source": [
        "np.random.seed(4)\n",
        "\n",
        "if len(positive_reviews) > len(negative_reviews):\n",
        "    np.random.shuffle(positive_reviews)\n",
        "    positive_reviews = positive_reviews[:len(negative_reviews)]\n",
        "else:\n",
        "    np.random.shuffle(negative_reviews)\n",
        "    negative_reviews = negative_reviews[:len(positive_reviews)]\n",
        "\n",
        "N = len(positive_reviews) +  len(negative_reviews)  # Number of samples"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP-Ph8ccWEE8"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()    # group same words e.g. cats & cat to reduce vocab size\n",
        "\n",
        "# convert sentence to tokens and filter out uninterested word\n",
        "def tokenize(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(s)\n",
        "    tokens = [t for t in tokens if len(t) > 2]      # only accept word with more than 2 letters\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQG11TsPTzLv"
      },
      "source": [
        "word_idx_map = {}   # word-index mapping\n",
        "last_idx = 0        # track index increment by word\n",
        "\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "\n",
        "# associate index to each vocab & tokenize sentences\n",
        "def learn_vocab(reviews, last_idx):\n",
        "    tokenized = []\n",
        "    for review in reviews:\n",
        "        tokens = tokenize(review.text)\n",
        "        tokenized.append(tokens)\n",
        "        for token in tokens:\n",
        "            if token not in word_idx_map:\n",
        "                word_idx_map[token] = last_idx\n",
        "                last_idx += 1\n",
        "    return tokenized, last_idx"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX17oE3iW4vY"
      },
      "source": [
        "positive_tokenized, last_idx = learn_vocab(positive_reviews, last_idx)\n",
        "negative_tokenized, last_idx = learn_vocab(negative_reviews, last_idx)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFppkQUDtcd4"
      },
      "source": [
        "def tokens_to_vector(tokens, label):\n",
        "    X = np.zeros(len(word_idx_map) + 1)      # vector size is # vocabs + 1 label\n",
        "    for t in tokens:\n",
        "        i = word_idx_map[t]\n",
        "        X[i] += 1\n",
        "    X = X/X.sum()                   # normalize word count in a sentence, so that all counts (vectors) sum up to 1\n",
        "    X[-1] = label\n",
        "    return X\n",
        "\n",
        "def fill_data(tokenized_reviews, last_idx, label):\n",
        "    for tokens in tokenized_reviews:\n",
        "        Xy = tokens_to_vector(tokens, label)\n",
        "        data[last_idx,:] = Xy\n",
        "        last_idx += 1\n",
        "    return data, last_idx"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dmOJJP5vJuz"
      },
      "source": [
        "data = np.zeros((N, len(word_idx_map) + 1))      # (# samples, # vocabs)\n",
        "last_idx = 0\n",
        "\n",
        "data, last_idx = fill_data(positive_tokenized, last_idx, 1)\n",
        "data, last_idx = fill_data(negative_tokenized, last_idx, 0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baAVIE6FwZxq",
        "outputId": "3c6ca04d-46c2-43c4-cb9c-3215ac91c3c3"
      },
      "source": [
        "np.random.shuffle(data)\n",
        "data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01639344, 0.01639344, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.        , 0.08333333, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        1.        ],\n",
              "       [0.04347826, 0.04347826, 0.        , ..., 0.        , 0.        ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brWySHnrhbzU"
      },
      "source": [
        "X_train = data[:-100,:-1]   # split last 100 rows as test set; last column is label\n",
        "X_test = data[-100:,:-1]\n",
        "y_train = data[:-100, -1] \n",
        "y_test = data[-100:, -1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtNB6a1liUDZ",
        "outputId": "8c91756d-b121-4636-eac8-a9e7afd3952a"
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeqZOx1pij5W",
        "outputId": "4c58862b-4b19-437a-8329-65c83fa014b2"
      },
      "source": [
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7878947368421053\n",
            "0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TGyjWq2kR4Z",
        "outputId": "213ec153-6f27-4a69-fa87-c5144e63b545"
      },
      "source": [
        "# Peek into word with high weightage in Logistic Reg Model\n",
        "\n",
        "THRESHOLD = 0.8\n",
        "for word, idx in word_idx_map.items():\n",
        "    weight = model.coef_[0][idx]\n",
        "    if weight > THRESHOLD or weight < -THRESHOLD:\n",
        "        print(f\"{word:<10}{weight}\")\n",
        "\n",
        "# sign in weightage = positive / negative sentimental\n",
        "\n",
        "# Further Effort:\n",
        "    # optimize threshold point in Logistic Regression            \n",
        "    # use more complex classification model or deep learning with RNN\n",
        "    # expand categories from binary to a scale of 3 or 5 points, middle be neutral\n",
        "    # increase sample size"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unit      -0.8867396209383875\n",
            "cable     0.8353727327798643\n",
            "sound     1.0468014306205844\n",
            "you       0.946293471324069\n",
            "n't       -1.9560568679723216\n",
            "easy      1.826783929118966\n",
            "quality   1.506312866080753\n",
            "item      -0.9013745300673538\n",
            "wa        -1.487048488648309\n",
            "perfect   1.033557004339358\n",
            "fast      0.8956123965812748\n",
            "price     2.635150400751305\n",
            "money     -1.0460526667278023\n",
            "memory    0.9896817080086341\n",
            "buy       -0.8988341609032072\n",
            "doe       -1.1774862573408704\n",
            "highly    0.930849641469327\n",
            "support   -0.8413442978671349\n",
            "little    0.9255377587475565\n",
            "excellent 1.3864532071684454\n",
            "love      1.2649520177015412\n",
            "poor      -0.8133698564336692\n",
            "then      -1.1154776158565054\n",
            "tried     -0.806217824199422\n",
            "speaker   0.9323279547383146\n",
            "return    -1.1439994196174428\n",
            "waste     -0.9945234711043558\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}